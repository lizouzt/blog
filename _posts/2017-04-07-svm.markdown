---
layout:     post    
title:      "支持向量机SVM简介"    
subtitle:   "统计机器学习之Support Vector Machine"          
date:       2017-04-07            
author:     "liujia"                      
comments:   true
header-img: "img/post-bg-04.jpg"
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

> 机器学习：对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E学习。

## 统计机器学习简介

统计机器学习是基于训练数据构建统计模型，从而对数据进行预测和分析，即学习（Learning）。 

统计学习分为，监督学习（supervised learning），非监督学习，半监督学习和强化学习（reinforcement learning），其中以监督学习最为常见和重要。

统计机器学习的过程如下： 

 > 1. 获取训练数据集合 
 > 2. 确定假设空间，即所有可能的模型的集合 
 > 3. 确定模型选择的准则（什么是最优模型的标准），即学习的策略 
 > 4. 实现求解最优模型的算法（如何获取最优模型），即学习的算法 
 > 5. 通过算法中假设空间中，找到最优模型 
 > 6. 使用该模型对新数据进行预测和分析


统计机器学习可以根据输入输出的不同类型，分为三类： 

```
* 分类问题 Classification
  输出变量是有限个离散值时，就是分类问题
  学习出的分类模型或分类决策函数称为分类器（classifier） 
  
* 回归问题 Regression
  输入输出都是连续变量值，就是回归问题，等价于函数拟合
  
* 标注问题 
  输入是一个观测序列，而输出是一个标记序列 
  典型的应用，词性标注，输入词序列，输出是（词，词性）的标记序列 
```    

统计机器学习三要素：

```
* 模型
* 策略
* 算法
```
可以这么认为 **学习 = 模型 + 策略 + 算法**


## 名词解释和记号
      
      

#### 输入数据：

$$(x_1,x_x,\ldots,x_p)^T$$
称之为一个样本（Sample）或者一个观测点，是p维列向量，每一个分量称为一个特征（Feature），容易看出共有p个特征。

对于监督学习而言，还要加上标注值y
$$(x_1,x_x,\ldots,x_p,y)^T$$

这样对于n个样本，我们得到输入矩阵
$$\left( \begin{array}{ccc}
x_{11} & x_{12} & ... & x_{1p} & y_{1} \\
x_{21} & x_{22} & ... & x_{2p} & y_{2} \\
\ldots\\
x_{n1} & x_{n2} & ... & x_{np} & y_{n} \end{array} \right)$$

#### 模型Model：

监督学习假设输入和输出的随机变量X和Y遵循联合概率分布P(X,Y)，P(X,Y)表示分布函数或分布密度函数。
 
这个是基本假设，即输入和输出数据间是存在某种规律和联系的，否则如果完全随机的，就谈不上学习了。并且训练数据和测试数据是依据P(X,Y)独立同分布产生的。

在监督学习中，可以将模型分为两类：概率模型和非概率模型

_非概率模型_

假设空间用 \\(\mathcal{F}\\) 表示，可以定义为决策函数的集合
$$ \mathcal{F} = \{f | Y = f(X)\}$$
其中X和Y是定义在输入空间\\(\mathcal{X}\\)和输出空间\\(\mathcal{Y}\\)上的变量，这时\\(\mathcal{F}\\)通常是由一个参数向量决定的函数族：
$$ \mathcal{F} = \{f | Y = f_{\theta}(X), \theta \subseteq R^n\}$$
参数向量\\(\theta\\)取值于n维欧式空间\\(R^n\\)，称为参数空间(Parameter Space)。

_概率模型_

假设空间也可以定义为条件概率集合
$$ \mathcal{F} = \{P | P(Y|X)\}$$
其中X和Y是定义在输入空间\\(\mathcal{X}\\)和输出空间\\(\mathcal{Y}\\)上的随机变量，这时\\(\mathcal{F}\\)通常是由一个参数向量决定的条件概率分布族：
$$ \mathcal{F} = \{P | P_{\theta}(Y|X), \theta \subseteq R^n\}$$
参数向量\\(\theta\\)取值于n维欧式空间\\(R^n\\)，也称为参数空间。

####策略Strategy：

_损失函数_

给定一个模型后，如何评价一个模型的好坏呢？

我们用损失函数（Loss Function）L(Y,f(X))来度量一次预测的结果好坏，即表示预测值f(X)和真实值Y之间的差异程度。


下面是几种常用的损失函数：


* 0-1损失函数
$$L(Y,f(X))=\left\{
\begin{aligned}
1 & , & Y=f(X) \\
0 & , & Y \neq f(X) \\
\end{aligned}
\right.$$

* 平方损失函数
$$L(Y,f(X))=(Y - f(X))^2$$

* 绝对损失函数
$$L(Y,f(X))=|Y - f(X)|$$

* 对数损失函数或者更一般的称为对数似然损失函数
$$L(Y,P(Y|X))=-\log(P(Y|X))$$


###### 风险和期望

_风险函数_ (Risk Function)或 _期望损失_ (Expected Loss) 

用于度量平均预测结果的好坏，其实就是损失函数的期望。

输入X，输出Y遵循联合分布P(X,Y)，定义如下:
$$R_{exp}(f) = E[L(Y,f(X))] = \int_{X*Y} (L(Y,f(X))*P(X,Y))\,dxdy$$

_经验风险_ (empirical risk)或 _经验损失_ (empirical loss) 

风险函数代表模型的平均的预测准确性，所以学习的目标应该是选择风险函数最小的模型。但是问题在于，计算风险函数需要知道联合分布P(X,Y)，但是这个是无法知道的，否则也不用学习了，所以我们无法精确的判断模型真正的效果。

既然无法得到真正的风险函数的值，我们就用训练数据集上的平均损失来模拟真正的风险函数
$$R_{exp}(f) = \frac{1}{N} \sum_{i=1}^n L(Y_i,f(X_i))$$

当N足够大的时候，根据大数定律，经验风险会逼近期望风险， 所以我们可以采用经验风险最小化的策略来找到最优的模型，当样本容量足够大的时候，能保证很好的学习效果，比如，常见的极大似然估计。
$$ \displaystyle \min_{f \subseteq \mathcal{F}}\frac{1}{N} \sum_{i=1}^n L(Y_i,f(X_i))$$

#### 算法
 
用什么样的方法来求解最优模型？ 

这样机器学习模型就归结为最优化问题，如果最优化问题有显式的解析解，比较简单，直接求出即可。但通常解析解是不存在的，所以就需要利用最优化算法来求解，比如将问题转化为凸优化问题(Convex Optimization)。

#### 其它
      
      
###### 过拟合

[这篇文档](https://en.wikipedia.org/wiki/Overfitting)

###### 正则化

[这篇文档](https://en.wikipedia.org/wiki/Regularization_(mathematics))

优先选择更简单的模型！





